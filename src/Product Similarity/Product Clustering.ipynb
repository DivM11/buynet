{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_clean(x):\n",
    "    if type(x) is str:\n",
    "        x = x.lower()\n",
    "        x = re.sub('<[^<]+?>', ' ', x)\n",
    "        x = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', x, flags=re.MULTILINE)\n",
    "        x = re.sub('[^A-Z a-z]+', ' ', x)\n",
    "        return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(words):\n",
    "    removed = [w for w in words if not w in stopWords]\n",
    "    \n",
    "    return removed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSymbols(words):\n",
    "    removed = [w for w in words if w.isalnum()]\n",
    "    \n",
    "    return removed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedSentence(sentence):\n",
    "    embed = map(embedding, sentence)\n",
    "    \n",
    "    return list(embed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word):\n",
    "    try:\n",
    "        embed = word_vectors[word]\n",
    "    except Exception:\n",
    "        embed = word_vectors['unk']\n",
    "        \n",
    "    return embed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageEmbedding(sentence):\n",
    "    average = np.mean(sentence, axis=0)    \n",
    "    return average;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(sentence, dataframe):\n",
    "    sentence = pd.Series(sentence, index=FeatureColumns)\n",
    "    dataframe = dataframe.append(sentence, ignore_index=True)\n",
    "    \n",
    "    return dataframe;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(r'scored_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(removal_list)):\n",
    "    sample = sample[sample['main_cat'] != removal_list[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = sample[['train_feature','main_cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF['train_feature'] = dataDF['train_feature'].apply(default_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF['train_feature'] = dataDF['train_feature'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF['train_feature'] = dataDF['train_feature'].apply(removeStopWords)\n",
    "dataDF['train_feature'] = dataDF['train_feature'].apply(removeSymbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddedDF = dataDF.copy()\n",
    "embeddedDF['train_feature'] = embeddedDF['train_feature'].apply(embedSentence)\n",
    "embeddedDF = embeddedDF[embeddedDF['train_feature'].astype(str)!='[]']\n",
    "keepIndices = embeddedDF.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddedDF['train_feature'] = embeddedDF['train_feature'].apply(averageEmbedding)\n",
    "embeddedDF.index = list(range(0, len(embeddedDF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureColumns = []\n",
    "for i in range(0, 300):\n",
    "    FeatureColumns.append('e'+str(i))\n",
    "FeatureDF = pd.DataFrame(columns = FeatureColumns)\n",
    "\n",
    "for i in range(0, len(embeddedDF['train_feature'])):\n",
    "    FeatureDF = featurize(list(embeddedDF['train_feature'][i]), FeatureDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(FeatureDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3)\n",
    "model = k_means.fit(X)\n",
    "y_hat = k_means.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['clusters'] = y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('scored_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureDF.to_csv('sentence_embeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
